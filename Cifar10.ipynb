{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preparing Data For Model Training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Helper functions\n",
    "def show_min_max(array, i):\n",
    "  random_image = array[i]\n",
    "  print(\"min and max value in image: \", random_image.min(), random_image.max())\n",
    "\n",
    "\n",
    "def plot_image(array, i, labels):\n",
    "  plt.imshow(np.squeeze(array[i]))\n",
    "  plt.title(str(label_names[labels[i]]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.show()\n",
    "\n",
    "# Variables to keep track of image size\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "# Variable for amount of output classes\n",
    "num_classes = 10\n",
    "\n",
    "# Set up array for labels:\n",
    "label_names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'] \n",
    "\n",
    "# Loading and backing up data\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "(train_images_backup, train_labels_backup), (test_images_backup, test_labels_backup) = cifar10.load_data()\n",
    "\n",
    "# Code to adjust the label arrays.\n",
    "train_labels_backup = [item for sublist in train_labels_backup for item in sublist] \n",
    "test_labels_backup = [item for sublist in test_labels_backup for item in sublist] \n",
    "\n",
    "# Printing out the data shape\n",
    "print(\"Training data shape:\",train_images.shape) \n",
    "print(\"Test data shape:\",test_images.shape) \n",
    "\n",
    "# Variable to keep track of input shape. \n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "# Print out the 100th image\n",
    "plot_image(train_images, 100, train_labels_backup)\n",
    "\n",
    "# Show it's min and max value\n",
    "show_min_max(train_images, 100)\n",
    "\n",
    "# Reformating the data to float32\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "\n",
    "# Normalize the values by dividing by 255\n",
    "train_images /= 255\n",
    "test_images /= 255\n",
    "\n",
    "# Convert labels using one-hot encoding. \n",
    "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "#change for your number of NVIDA GPUs (if you have more than one)\n",
    "config = tf.compat.v1.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \n",
    "sess = tf.compat.v1.Session(config=config) \n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Change this to change accuracy\n",
    "# The more Epochs the higher the accuracy and longer training times\n",
    "# Less Epochs means faster trainng times but lower accuracy\n",
    "epochs = 20\n",
    "\n",
    "batch_size = 128\n",
    "model = Sequential()\n",
    "\n",
    "#Layer 1\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) \n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#Layer 2\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) \n",
    "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#Layer 3\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) \n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) \n",
    "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#Layer 4\n",
    "model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) \n",
    "model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) \n",
    "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#Output Layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Compiling and Training Network\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(test_images, test_labels), shuffle=True)\n",
    "scores = model.evaluate(test_images, test_labels,verbose=0)\n",
    "print('Test accuracy:', scores[1])\n",
    "model.save(\"Cifar10_Model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "num = 0\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "(train_images_backup, train_labels_backup), (test_images_backup, test_labels_backup) = cifar10.load_data()\n",
    "\n",
    "def GetRandomImage():\n",
    "    global num\n",
    "    num = random.randint(0, 10000)\n",
    "    plt.imshow(test_images[num])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.savefig(\"./img.png\", bbox_inches='tight',pad_inches = 0)\n",
    "    plt.show()\n",
    "    \n",
    "GetRandomImage()\n",
    "# Disable scientific notation for clarity\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Load the model\n",
    "model = tensorflow.keras.models.load_model('Cifar10_Model.h5')\n",
    "labels = [line for line in open(\"CifarLabels.txt\")]\n",
    "\n",
    "# Create the array of the right shape to feed into the keras model\n",
    "# The 'length' or number of images you can put into the array is\n",
    "# determined by the first position in the shape tuple, in this case 1.\n",
    "data = np.ndarray(shape=(1, 32, 32, 3), dtype=np.float32)\n",
    "\n",
    "# Replace this with the path to your image\n",
    "image = Image.open('img.png')\n",
    "\n",
    "#resize the image to a 224x224 with the same strategy as in TM2:\n",
    "#resizing the image to be at least 224x224 and then cropping from the center\n",
    "size = (32, 32)\n",
    "image = ImageOps.fit(image, size, Image.ANTIALIAS)\n",
    "\n",
    "#turn the image into a numpy array\n",
    "image_array = np.asarray(image)\n",
    "\n",
    "# display the resized image\n",
    "\n",
    "# Normalize the image\n",
    "normalized_image_array = (image_array.astype(np.float32) / 255) - 1\n",
    "\n",
    " # Load the image into the array\n",
    "data[0] = test_images[num]\n",
    "\n",
    "# run the inference\n",
    "prediction = model.predict(data)\n",
    "print(\"\\nComputer thinks this is a:\", labels[np.argmax(prediction)])\n",
    "actualindex = str(test_labels[num])\n",
    "actualindex = actualindex.replace(\"[\", \"\", 1)\n",
    "actualindex = actualindex.replace(\"]\", \"\", 1)\n",
    "print(\"This is actually a:\", labels[int(actualindex)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
